# Unsupervised Detection of Exoplanetary Signatures in ALMA Continuum Data

**Note to Judges:** To run this pipeline, please place the evaluation `.fits` data cubes into the `/general_test/data/` directory located at the general_test directory of this project. This folder is intentionally tracked in `.gitignore` to prevent large observational files from being pushed to the repository. The data loader is pre-configured to automatically ingest `*.fits` files from this directory.

---

## 1. Problem Statement & Pipeline Architecture

The objective of this project is to perform unsupervised clustering on synthetic Atacama Large Millimeter/submillimeter Array (ALMA) continuum observations of protoplanetary disks. The primary goal is to identify structural signatures of hidden exoplanets—such as gaps, rings, or localized deviations from Keplerian velocity.

The assignment enforces strict constraints:

1. **No labels are provided**, prohibiting the use of supervised classification models.
2. The model must **avoid clustering simply by viewing angle** (inclination).
3. The pipeline must isolate the relevant observational data from multidimensional data cubes.

To meet these constraints, we designed an end-to-end unsupervised pipeline consisting of four stages: Logarithmic Data Preprocessing, Geometric Invariance Transformations, Feature Extraction via a Convolutional Autoencoder (CAE), Dimensionality Reduction (UMAP), and Density-Based Clustering (HDBSCAN).

---

## 2. Data Ingestion & Preprocessing

The raw synthetic data consists of 4-layer $600 \times 600$ spatial resolution `.fits` data cubes.

### Stokes Parameter Isolation

The FITS headers define the four layers as the Stokes parameters (`I = total flux`, `Q`, `U`, and `V`). The data loader explicitly slices `hdul[0].data` to extract only the 0th index (Stokes I). This isolates the total continuum emission—which contains the physical disk morphology—and discards the magnetic polarization states (Q, U, V) which are confounding variables for structural clustering.

### The Logarithmic Stretch (`np.log1p`)

Protoplanetary disks possess an extreme dynamic range: the central star's emission is orders of magnitude brighter than the faint planetary wakes at the disk's outer edges. If standardized using a simple Min-Max scaler, an autoencoder suffers from a severe "brightness bias," dedicating its entire learning capacity to minimizing the Mean Squared Error (MSE) of the bright center while mathematically ignoring the faint gaps.

By applying a logarithmic stretch (`np.log1p`) after shifting the baseline to zero, the pipeline compresses this dynamic range. This dims the central flux and mathematically boosts the signal of the faint outer rings, forcing the neural network to weight the subtle planetary structural deviations equally with macroscopic disk features.

---

## 3. Geometric Transformations (Mitigating Inclination Bias)

The problem statement explicitly warns against clustering by viewing angle. An autoencoder fed raw images will naturally separate face-on (circular) disks from highly inclined, edge-on (elliptical) disks due to massive pixel variance.

To combat this, two transformations are applied:

1. **`CenterCrop(512)`:** Convolutional downsampling requires perfectly divisible spatial dimensions. Cropping the $600 \times 600$ tensor to $512 \times 512$ (a power of 2) safely removes the empty deep-space margins while preventing fractional tensor mismatches during the upsampling phase.
2. **`RandomRotation(180, fill=0)`:** To force rotational invariance, every image undergoes a random continuous rotation during training. This forces the autoencoder to map identical gap structures to the exact same latent space coordinates, regardless of their azimuthal orientation. The `fill=0` argument ensures that empty corners generated by the rotation matrix are padded with true background noise (0) rather than sharp geometric artifacts.

---

## 4. Deep Learning Architecture: Convolutional Autoencoder (CAE)

To autonomously extract structural features, we developed a deep Convolutional Autoencoder. This network learns a localized morphological "fingerprint" by compressing the spatial data through a strict continuous bottleneck.

### Architectural Details

- **The Encoder:** Utilizes 5 progressive `Conv2d` layers (`kernel_size=3`, `padding=1`, `stride=2`). The `stride=2` argument halves spatial dimensions at each layer while the channel depth expands, trading raw pixels for complex geometric features. `BatchNorm2d` is applied sequentially to stabilize gradients.
- **The Latent Bottleneck:** The $256 \times 16 \times 16$ feature map is flattened and compressed via a `Linear` layer into a **512-dimensional continuous latent vector**. This vector acts as the structural representation of the disk, stripped of noise.
- **The Decoder:** The vector is unflattened and passed through 5 `ConvTranspose2d` layers. Using `stride=2` and `output_padding=1`, it perfectly doubles resolution at each step, culminating in a `Sigmoid` activation for MSE Loss calculation.

---

## 5. Dimensionality Reduction (UMAP)

The 512-dimensional latent space suffers from the "curse of dimensionality," rendering traditional distance metrics (e.g., Euclidean distance) ineffective for clustering.

Uniform Manifold Approximation and Projection (UMAP) is deployed to project the 512D vectors down to a 2D topology. Hyperparameters were carefully tuned (`n_neighbors=15`, `min_dist=0.1`) to relax the manifold. This ensures UMAP preserves both local relationships (disks with similar gap structures group tightly) and global relationships (smooth disks are broadly separated from multi-ring disks).

---

## 6. Density-Based Clustering (HDBSCAN)

To assign discrete categories to the continuous UMAP projection without relying on human priors, Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) is utilized.

- **Density over Geometry:** Unlike K-Means, which forces spherical geometries and requires a pre-determined cluster count ($K$), HDBSCAN scans the 2D space for continuous regions of high data density. It organically determines the number of distinct planetary configurations present in the dataset.
- **Anomaly Filtering:** Synthetic observational data contains varying levels of noise. If the autoencoder maps a highly chaotic or corrupted disk structure that does not align with a dominant manifold, HDBSCAN formally isolates it as noise (Cluster `-1`). This prevents anomalous data from polluting the primary clusters of scientifically viable exoplanetary systems.

## Conclusion

By engineering strict controls for brightness bias and inclination geometry, and by forcing the observational data through a dense 512-dimensional structural bottleneck, this pipeline successfully and autonomously isolates disks containing planetary wakes from featureless disks, fulfilling all assignment deliverables without the use of supervised labels.

```

```

```

```
